---
layout: post
title: RL 1. Intro
permalink: rl-1-intro
feature-img: "assets/images/rl/agent-env-interaction.jpeg"
tags: machine-learning reinforcement-learning
---

This is the place for the excerpt if we change show_excerpts to true in `_config.yml`
<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

# RL tasks intro

![]({{ site.baseurl }}/assets/images/rl/agent-env-interaction.jpeg) The agent–environment interaction. EN LA IMAGEN FALTA LA STATE-VALUE FUNCTION Y PHYSICS OR DYNAMICS IS THE state transition probability matrix
{:.figure}

In Reinforcement learning (RL) we stablish a **frontier** that separates a region of the universe that we'll call the **agent**, from the rest of the surrounding universe, thar we'll call its **environment**. Depending on the **task** we are trying to solve, the place where we establish this frontier will change, and with it, our definition of agent. The interaction between the agent and the environment can be diecretized in **time steps** $$t$$. In order to be able of solve the task, the agent must be able to:

- on each time step, make an (at least partial) **observation** ($$O_t$$) of the **state** ($$S_t$$) of its environment
- for each state, perform an **action** ($$A_t$$) on its environment according to some **policy** ($$\pi$$) that fully defines the behaviour of the agent
- for each action, receive a scalar feedback signal from its environment, or **reward** ($$R_t$$)
- implement a **learning method** to modify its policy in order to achieve its **goal** (maximize the **return** ($$G_t$$), i.e., the expected cumulative value of future rewards)

We can formally define a policy ($$\pi$$) as:

A **policy** $$\pi$$ is formally is a distribution over all actions ($$a$$) in the set of possible actions $$\mathcal{A}$$ given any state ($$s$$) from the set of possible environment states ($$\mathcal{S}$$):

$$
\pi(a \mid s) \doteq \mathbb{P}\left[A_{t}=a \mid S_{t}=s\right]
$$

Sometimes the agent's learning method can make use of a **model** of the environment (an aproximation environment physics or dynamics) in which case, the learning algorithm is known as **model-based**, or as **model-free** if does not use of a model of the environment. 

Sometimes the learning method can also make use of a **state-value function**, a function that maps each state ($$s$$) to the return the agent can expect following its policy after that state:

$$
v_{\pi}(s) \doteq \mathbb{E}\left[G_{t} \mid S_{t}=s\right] \qquad \forall s \in \mathcal{S}
$$

Whereas rewards determine the immediate, intrinsic desirability of environmental states, state-values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states. For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards (or the reverse could be true). 

## Optimality

The maximum value of a state that can be achieved by any policy is known as the **optimal state-value function**:

$$
v_{*}(s) \doteq \max _{\pi} v_{\pi}(s) \qquad \forall s \in \mathcal{S}
$$

Value functions define a **partial ordering** over policies. A policy $$\pi$$ is defined to be better than or equal to a policy $$\pi^{\prime}$$ iff its expected return is greater than or equal to that of $$\pi^{\prime}$$ for all states:

$$
\pi \geq \pi^{\prime} \quad \text { if } \quad v_{\pi}(s) \geq v_{\pi^{\prime}}(s) \qquad \forall s \in \mathcal{S}
$$

There is always at least one policy that is better than or equal to all other policies, and is called an **optimal policy**. Although there may be more than one, all the optimal policies are denoted by $$\pi_{*}$$. Any optimal policy achieves the optimal state-value for all states:

$$
v_{\pi_*}(s) = v_*(s) \geq v_{\pi^{\prime}}(s) \qquad \forall s \in \mathcal{S}
$$

The final agent learning problem is then learning what to do—how to map the different environment states to actions—so as to maximize the numerical reward signal, or more formally, learning a policy that yields the maximum return for each state, i.e., an optimal policy.

> The RL problem is solved finding an optimal policy.

## RL and the other machine learning paradigms

The RL problem is different from the other machine learning paradigms:

- The ***Supervised learning*** (SL) problem is to generalize (or extrapolate) a partial policy provided by some knowledgable external supervisor (training set), to observations not present in the given policy.
- The ***Unsupervised learning*** (UL) problem is typically about finding structure hidden in collections of unlabeled observations, where unlabeled refers to the absence of a correspondant action. (X's without the Y's). 
- The ***Reinforcement learning*** problem is to obtain an optimal policy from experience (observations of the environment, and the actions taken to modify it).

but these three learning paradigms, can help the learning agent to find the **optimal policy** by using a provided partial policy (SL), structure found in a collection of observations (UL), or from experience with the environment (RL). 

Learning from experience presents two important distinguishing features of RL:

- Agent’s actions ussually affect the subsequent data it receives therefore:
	- The training dataset is not fixed
	- Feedback is delayed, not instantaneous. Actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards
	- The data the agent receives is not i.i.d. It's sequential (respects temporal continuity)
- Unlike supervised learning, in RL feedback is evaluative, not instructive (there are no labels, only a reward signal)
  - **Instructive feedback** tells the agent how to achieve its goal (the optimal action), regardless of which action has been taken
  - **Evaluative feedback** tells the agent only in what measure it has achieved its goal given the action taken (reward signal), but no information about how to achieve it.
  - Evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken. 
  - Un ejemplo sería que alguien me pregunte qué estoy mirando, y le hago jugar al veo veo con "frio, tibio, caliente" (evaluative), o le digo directamente qué estoy mirando (instructive).
  - This is what creates in RL the need for active exploration, for an explicit trial-and-error search for good behavior, and the consecuent exploration-exploitation dilemma. We will analyze this in detail in the [next chapter]({{ 'rl-2-bandit-problems' | relative_url }}).

## The reward hypothesis

It is important to note that we defined the agent's goal not as solving the task we want to solve, but as maximizing the return without talking at all about the details of the task. 

The **reward hypothesis** states that:

> Any task that we want to solve, can be described to the agent just as the maximization of the return. 

The reward hypothesis is so central to RL, that we'll make some comments:

* It is important to remember that the goal of the agent is then not to maximize immediate reward that receives after the action taken, and not to solve the task we have in mind, but just to maximize the return
* It may be sometimes better for the agent to sacrifice immediate reward to gain more long-term reward
* It is then critical to design the reward signal in such a way that maximizing the return, the agent solves the task
* The reward signal is not the place to impart to the agent prior knowledge about how to solve the task (better places for imparting this kind of prior knowledge are the initial policy or the initial state-value function). The reward signal is your way of communicating to the agent what you want achieved, not how you want it achieved.
* Although describing any task in terms of just a reward signal might at first appear limiting, in practice it has proved to be flexible and widely applicable

## Episodic and continuing tasks

As we said earlier, the interaction between the agent and the environment can be diecretized in **time steps** $$t$$, and at each time step:

- the agent makes an obervation ($$O_t$$), performs an action ($$A_t$$), and receives a reward ($$R_t$$) and an observation ($$O_{t+1}$$). 
- the environment receives an action ($$A_t$$), and emits a new obervation ($$O_{t+1}$$), and a reward ($$R_{t+1}$$)

This generate a sequence known has **history**:

$$
H_t = O_{0}, A_{0}, R_{0}, O_{1}, A_{1}, R_{1}, \dots, O_{t-1}, A_{t-1}, R_{t-1}, O_{t}, A_{t}, R_{t}
$$

If the task does not have a defined ending, this sequence goes on forever, and the task is known as a **continuing task**. 

On the other hand, if the task has a defined ending (player wins a board game, robot runs out of battery, etc.), the sequence ends at the time known as **time of termination** $$T$$. These tasks are called **episodic tasks** and this finite history an **episode**:

$$
episode = H_T = O_{0}, A_{0}, R_{0}, O_{1}, A_{1}, R_{1}, \dots, O_{T-1}, A_{T-1}, R_{T-1}, O_{T}, A_{T}, R_{T}
$$

## Return and discounting

The expected cumulative value of future rewards or **return**, is formally defined as some specific function of the reward sequence $$R_{t+1}, R_{t+2}, R_{t+3}, \dots$$. 

We will define it in the simplest way just as just the sum of the rewards:

$$
G_{t} \doteq R_{t+1}+R_{t+2}+R_{t+3}+\dots
$$

The definition of $$G_{t}$$ is problematic for continuing tasks because the final time step would be $$T=\infty$$, and the return, which is what we are trying to maximize, could easily be infinite. Thus, we can make this definition slightly more [complex conceptually](#why-discounting) but mathematically more stable, introducing a parameter $$\gamma \in[0,1]$$ called the **discount rate**, and defining the **discounted return** as:

$$
G_{t} \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}
$$

Now, if $$\gamma<1$$, the discounted return in has a **finite value** as long as the reward sequence {$$R_k$$} is **bounded**.

* The parameter $$\gamma$$ determines the **present value of future rewards**. 
* A reward received $$k$$ time steps in the future is worth only $$\gamma^{k−1}$$ times what it would be worth if it were received immediately.
* If $$\gamma = 0$$, the agent is **“myopic”** in being concerned only with maximizing immediate rewards $$R_{t+1}$$. 
* As $$\gamma$$ approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more **farsighted**.

For all time steps $$t < T$$, (even if termination occurs at $$t+1$$, if we define $$G_T = 0$$), returns at successive time steps are related to each other in a **recursive** way that often makes it easy to compute returns from reward sequences:

$$
\begin{aligned} G_{t} & \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} R_{t+4}+\cdots \\ &=R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\gamma^{2} R_{t+4}+\cdots\right) \\ &=R_{t+1}+\gamma G_{t+1} \end{aligned}
$$

# Environment formalization. MDP's.

## States

Formally, a state $$S_{t}$$ is a function of the history:

$$
S_{t}=f\left(H_{t}\right)
$$

All the information of the history used by the agent to make decisions is known as the **agent state** ($$S_{t}^{a}$$), and all the information of the history used by the environment to determine the reward and the next observation is known as the **environment state** ($$S_{t}^{e}$$). Being states, both are a function of the history.

### Markov state

> The future is independent of the past given the present.

We call a any environment state ($$S_t$$) a **Markov state** or **information state** iff it satisfies the **Markov property**. This property establishes that in the state $$S_t$$ we have all the information about past states needed to determine the future states. More formally, **Markov property** declares that the conditional probability distribution of future states of the sequence (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it (past states):

$$
\mathbb{P}\left[S_{t+1} \mid S_{t}\right]=\mathbb{P}\left[S_{t+1} \mid S_{1}, \ldots, S_{t}\right]
$$

A Markov state contains all useful information from the history (is a sufficient statistic of the future), therefore, once this state is known, the ***history may be thrown away***.

### State transition probability matrix

For a Markov state $$s$$ and successor state $$s^{\prime}$$, the **state transition probability matrix** is defined by:

$$
\mathcal{P}_{\mathrm{ss}^{\prime}}=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s\right]
$$

The state transition probability matrix $$\mathcal{P}$$:

$$
\mathcal{P}=\left[\begin{array}{ccc}
\mathcal{P}_{11} & \dots & \mathcal{P}_{1 n} \\
\vdots \\
\mathcal{P}_{n 1} & \dots & \mathcal{P}_{n n}
\end{array}\right]
$$

defines transition probabilities from all states $$s$$ to all successor states $$s^{\prime}$$. Note that each row of the matrix sums to 1.

## Processes

### Markov processes

A **Markov process (MP)** (or Markov chain) is a stochastic model describing a **sequence of stochastic states** that satisfy the Markov property. It is a tuple $$\langle\mathcal{S}, \mathcal{P}\rangle$$ where $$\mathcal{S}$$ is a **finite** set of Markov states, and $$\mathcal{P}$$ the state transition probability matrix between those states.

### Markov reward processes

A **Markov reward process (MRP)** is a Markov chain with values, i.e., a tuple $$ \langle\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma\rangle $$ where in addition to the **finite** set of Markov states ($$\mathcal{S}$$), and the state transition probability matrix ($$\mathcal{P}$$), we add the [discount factor](#return-and-discounting) $$\gamma$$, and the reward function $$\mathcal{R}$$ with: 

$$
\mathcal{R}_{s}=\mathbb{E}\left[R_{t+1} \mid S_{t}=s\right]
$$

### Markov decision process

A **Markov decision process (MDP)** is a tuple $$ \langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma\rangle $$ where compared to an MRP we add a **finite** set of actions $$\mathcal{A}$$, and modify the reward function $$\mathcal{R}$$ to:

$$
\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]
$$

and the state transition probability matrix ($$\mathcal{P}$$) to:

$$
\mathcal{P}_{\mathrm{ss}^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right]
$$

MDP's generalizes directly the previous concepts of MP, and MRP:

* A MRP can be thought as a MDP where only one action exists for each state (e.g. "wait").
* A MP can be thought as a MRP where all rewards are the same (e.g. "zero").

MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. As in all of artificial intelligence, there is a tension between breadth of applicability and mathematical tractability. Despite its considerable level of abstraction, the MDP framework offers a great a flexibility to be applied to many different problems in many different ways, and has proved to be widely useful and applicable.

#### Observability

When we talk about MDP's we imply the concept of **full observability**, i.e., that the agent directly observes [environment state](#states), then:

$$
O_{t}=S_{t}^{a}=S_{t}^{e}
$$

And therefore, the definition of **history** becomes:

$$
H_t = S_{0}, A_{0}, R_{0}, S_{1}, A_{1}, R_{1}, \dots, S_{t-1}, A_{t-1}, R_{t-1}, S_{t}, A_{t}, R_{t}
$$

If the agent can only **partially** observe the full environment state ($$S_{t}^{a}\neqS_{t}^{e}$$) the process is formally known as **partially observable Markov decision process (POMDP)**. We are not going to deal with POMDP's because, usually, PO problems can be [converted into MDPs](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process#Belief_MDP).

#### Dynamics function

In a **finite** MDP, we can define the **dynamics function** $$p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow[0,1]$$:

$$
p\left(s^{\prime}, r | s, a\right) \doteq \mathbb{P}\left[S_{t+1}=s^{\prime}_{,} R_{t+1}=r \mid S_{t}=s, A_{t}=a\right]
$$

This ordinary **deterministic** function of four arguments **completely characterize the environment’s dynamics**, and from it we can obtain:

$$
\mathcal{P}_{\mathrm{ss}^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right] = \sum_{r} p\left(s^{\prime}, r \mid s, a\right)
$$

and:

$$
\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right] = \sum_{r} r \sum_{s^{\prime}} p\left(s^{\prime}, r \mid s, a\right)
$$

Given an MDP $$\mathcal{M}=\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma\rangle$$ and a policy $$\pi$$ and defining:

$$
\mathcal{P}_{s, s^{\prime}}^{\pi}=\sum_{a} \pi(a \mid s) \mathcal{P}_{s s^{\prime}}^{a} \qquad \text{and} \qquad \mathcal{R}_{s}^{\pi}=\sum_{a} \pi(a \mid s) \mathcal{R}_{s}^{a}
$$

* The state sequence $$S_{1}, S_{2}, \ldots$$ is a MP $$\left\langle\mathcal{S}, \mathcal{P}^{\pi}\right\rangle$$.
* The state and reward sequence $$S_{1}, R_{2}, S_{2}, \ldots$$ is a MRP $$\left\langle\mathcal{S}, \mathcal{P}^{\pi}, \mathcal{R}^{\pi}, \gamma\right\rangle$$

# Bellman equations

The **state-value** function $$v(s)$$ of an MRP is the expected return starting from state $$s$$:

$$
v(s)=\mathbb{E}\left[G_{t} \mid S_{t}=s\right]
$$

As we saw earlier, $$G_t$$ can be expressed as $$G_t = R_{t+1}+\gamma G_{t+1}$$, then we have:

$$
v(s)=\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right]
$$

Using the linearity of the expectation we have:

$$
\begin{aligned}
v(s)&=\mathbb{E}\left[R_{t+1}\mid S_{t}=s\right] + \gamma \mathbb{E}\left[G_{t+1} \mid S_{t}=s\right]
\\ &= \mathbb{E}\left[R_{t+1}\mid S_{t}=s\right] + \gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}} \mathbb{E}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right]
\end{aligned}
$$

where the first term is the definition of $$\mathcal{R}_{s}$$ and $$ \mathbb{E}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right]$$ is the definition of $$v(s^{\prime})$$ therefore, $$v(s)$$ can be obtained recursively in what  is known as the **Bellman equation for state-values in a MRP**:

$$
v(s)=\mathcal{R}_{s}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}} v\left(s^{\prime}\right)
$$

which can be quickly expressed in matrix form, defining the column vector $$v$$ with one entry per state:

$$
v=\mathcal{R}+\gamma \mathcal{P} v
$$

As we saw, a MDP can be turn into a MRP by [setting a policy](#policy) $$\pi$$. Then, the **Bellman equation for state-values in a MDP**, is just the equation for the MRP, but with $$\pi$$ as an index where corresponds:

$$
\begin{aligned}
v_{\pi}(s)&=\mathcal{R}^{\pi}_s+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}^{\pi}_{s s^{\prime}} v_{\pi}\left(s^{\prime}\right)
\\ &= \sum_{a} \pi(a \mid s)\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right)
\\ &=\sum_{a} \pi(a | s) \left( \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right]\right)
\end{aligned}
$$

Where in the second line we just used the definitions of $$\mathcal{R}^{\pi}_s$$ and $$\mathcal{P}^{\pi}_{s s^{\prime}}$$, and in the last line, the definitions of $$\mathcal{R}^{a}_s$$ and $$\mathcal{P}^{a}_{s s^{\prime}}$$.

We can also express this equation in matrix form:
$$
v_{\pi}=\mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} v_{\pi}
$$

Being a linear problem, we have a direct solution:

$$
v_{\pi}=\left(I-\gamma \mathcal{P}^{\pi}\right)^{-1} \mathcal{R}^{\pi}
$$

This solution has a computational complexity of $$O(n^3)$$ for $$n$$ states, therefore a direct solution is only possible for small MRPs, and for larger cases have been developed many iterative methods that we will talk about in detail later.

**TODO**: code example
{:.message}

## Existence and uniqueness

The existence and uniqueness of $$v_{\pi}$$ are guaranteed as long as either $$\gamma<1$$ or eventual termination is guaranteed from all states under the policy $$\pi$$.

## Action-value function

Sometimes, for the learning method, is useful to define an **action-value function**, denoted as $$q_{\pi}(s, a)$$ ([why q?](https://stats.stackexchange.com/a/346587)), as the expected return starting from $$s$$, taking the action $$a$$, and following the policy $$\pi$$ thereafter:

$$
q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s, A_{t}=a\right]
$$

Similarly to the [optimal state-value function](#(#optimality)), we have the **optimal action-value** function, denoted $$q_{*}$$, and defined as maximum action-value function over all policies:

$$
q_{*}(s, a) \doteq \max _{\pi} q_{\pi}(s, a) \qquad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)
$$

And also, any optimal policy achieves the optimal action-value for all states and actions:

$$
q_{\pi_*}(s, a) = q_*(s, a) \geq q_{\pi^{\prime}}(s, a) \qquad \forall s \in \mathcal{S} \quad \forall a \in \mathcal{A}
$$

The optimal action-value function $$q_*$$ gives us for any state of the MDP, and any action, the maximum return we can extract by following any policy, so it specifies directly the optimal way to behave in the MDP. We can say that when we know the optimal action-value function, the RL problem is solved, existing always the deterministic optimal policy:

$$
\pi_{*}(a \mid s)=\left\{\begin{array}{ll}
1 & \text { if } a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} \left(q_{*}(s, a)\right) \\
0 & \text { otherwise }
\end{array}\right.
$$

## Backup diagrams

Similarly to how we did with the state-value function, we can express the action-value function as:
$$
\begin{aligned}
q_{\pi}(s, a) &= \mathbb{E}_{\pi}\left[R_{t} + \gamma G_{t+1} \mid S_{t}=s, A_{t}=a\right]
\\ &=\mathbb{E}_{\pi}\left[R_{t+1}\mid S_{t}=s, A_{t}=a\right] + \gamma \mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t}=s, A_{t}=a\right]
\\ &= \mathbb{E}_{\pi}\left[R_{t+1}\mid S_{t}=s, A_{t}=a\right] + \gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}^a_{s s^{\prime}} \mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right]
\\ &=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)
\end{aligned}
$$
and make explicit how to obtain $$q_{\pi}$$ from the state-function $$v_{\pi}$$. For completeness, we will also make explicit how to obtain $$v_{\pi}$$ from $$q_{\pi}$$, which is quite straightforward if we notice that what multiply $$\pi(a \mid s)$$ in the Bellman equation for $$v_{\pi}$$ is just the expression we obtained in the previous equation for $$q_{\pi}(s, a)$$:
$$
v_{\pi}(s)= \sum_{a} \pi(a \mid s) q_{\pi}(s, a)
$$
We can make use to graphically express the two last equations the diagrams shown in the next picture, and known as **backup diagrams**. Each open circle represents a state and each solid circle represents a state–action pair. The links represents the look ahead of the possible actions in the case of the state-values, and the look ahead of the possible next states, in the case of the action-values. As you can guess, the diagram on the left represents how to obtain $$v_{\pi}$$ from $$q_{\pi}$$, and the diagram on the right, the opposite relationship.

![][backup-diagram] State-value and action-value backup diagrams.
{:.figure}

Combining the two previous equations, we can obtain a recursive calculation of $$q_{\pi}$$, known as the **Bellman equation for action-values in a MDP**:

$$
q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime}} \mathcal{P}_{s s^{\prime}}^{a} \sum_{a^{\prime}} \pi\left(a^{\prime} \mid s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)
$$

We can also make use of the backup diagrams to represent the Bellman equations for the state-values (on the left), and the action-values (on the right).

![][bellman-backup-diagrams] State-value and action-value Bellman equations backup diagrams.
{:.figure}

## Bellman optimality equations

Using the equations that relate $$v_{\pi}$$ and $$q_{\pi}$$ for the optimal values, we have:

$$
v_{*}(s)=v_{\pi_*}(s)=\pi_*(a \mid s) \; q_{\pi_*}(s, a) = \max _{a} q_{*}(s, a)
$$

And:

$$
q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{\text {ss' }}^{a} v_{*}\left(s^{\prime}\right)
$$

Which we can express through backup diagrams, using the arc between branches to symbolize the maximum function:

![][optimality-backup-diagrams] Optimal state-value and action-value backup diagrams.
{:.figure}

Using the two previous equations, we can write the **Bellman optimality equations** for the state, and the action-values in a MDP:

$$
v_{*}(s)=\max _{a} \left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)\right)
$$

$$
q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)
$$

and show them in the respective backup diagrams.

![][bellman-optimality-backup-diagrams] State-value and action-value Bellman optimality equations backup diagrams.
{:.figure}

After all the definitions in this chapter, we can say that **solving the RL problem just boils down to solve the Bellman optimality equations**. BUT, as you can see, these equations are **non-linear** (both take a max over rewards, or $$q_*$$ values) and therefore, there is no closed form solution (in general) as in the case of the non-optimal Bellman equations. 

## Solving methods

In order to solve these non-linear equations, several algorithms have been developed and we will discuss them in detail in the following chapters.

These methods can be classified by whether or not they use an explicit policy, the state-value function, or the [dynamics function](#dynamics-function) $$p$$ (or an approximation) known as a **model** of the environment, and are known as **policy/value/model-based** respectively. Methods that do not use any of these functions explicitly, are simply known as **policy/value/model-free** depending on the case. For example a learning method that doesn't use a state-value function and directly improves its policy, is known as value-free method. As we study the different learning methods, we will revisit this classification. We will also cover a particular type of methods that use both a policy and a value function, known as action-value methods. The graphic representation of this classification can be observed in the diagram on the left.

![][rl-algorithm-categorization] Solving methods classification (left) Model-based and direct (model-free) RL learning flow (right). [source](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)
{:.figure}

In model-free methods, with the information that the agent obtains when acting on the environment, it directly updates its policy and/or its value function, in a process known as **direct-RL**. On the contrary, in the case of model-based methods, the learning process can be divided into two stages known as model learning and planning (diagram on the right). By acting on the environment, and *observing* the new state of the modified environment (and the possible reward), the agent produces a wealth of information about cause and effect, the consequences of actions, etc. that is used to update the model (**model learning**), and then this model is used to update the policy/value function more efficiently that with the direct samples of the environment in the process commonly known as **planning**. 

**NOTE**: Here we are going to focus on the planning stage, and we will not delve into the model learning stage. To delve into the details of the model learning stage, a good starting point is [Moerland et al. (2020)](Model-based Reinforcement Learning: A Survey)
{:.message}

If the dynamics function $$p$$ is directly given to the agent, the model is said to be "known", and it doesn't have to be learned. In this case, then, we can arrive to an optimal policy (solve the MDP) simply by planning. One way to achieve it is by using a mathematical optimization method known as **dynamic programming**.

**TODO**: add maze example? from https://www.davidsilver.uk/wp-content/uploads/2020/03/intro_RL.pdf
{:.message}

# Planning with Dynamic programming

The key idea of **dynamic programming (DP)** is to simplify the resolution of a complicated problem by breaking it down into simpler sub-problems and then recursively finding the optimal solutions to the sub-problems. If a problem has a recursive structure such that it can be solved optimally in this way, then it is said to have **optimal substructure**. As we saw when deriving the [Bellman equations](#bellman-equations), MDP's are a perfect example of such type of problems.

## Policy evaluation or prediction

The first step of DP is to **estimate iteratively** the value of an initial policy in a process called **prediction**. 

We saw that we can obtain the state-value of a given policy at a certain state using the Bellman equation:

$$
v_{\pi}(s)=\sum_{a} \pi(a | s) \left( \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right]\right)  \qquad \text{or in matrix form} \qquad v_{\pi} = \mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} v_{\pi}
$$

and that since it constitutes a linear problem, it can be solved directly, but with a computational complexity of $$O(n^3)$$ for $$n$$ states. 

> The clever workaround of dynamic programming is to convert the Bellman equation in an **iterative** method.

By doing this, the computational complexity of DP is $$O(mn^2)$$ for $$m$$ actions and $$n$$ states, which is a reduction since generally an MDP has more states than possible actions. The computational complexity of DP for action-values is $$O(m^2n^2)$$.

The Bellman equation can be used to obtain a sequence of approximations ($$v_{0}, v_{1}, v_{2}, \dots$$) of the value function $$v_{\pi}$$, where the initial approximation ($$v_0$$) is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman equation as an update rule:

$$
v_{k+1}(s)=\sum_{a} \pi(a | s) \left( \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{k}\left(s^{\prime}\right)\right]\right)  \qquad \text{or in matrix form} \qquad v_{k+1} = \mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} v_{k}
$$

* Each iteration of the policy evaluation updates the value of every state once to produce the new approximate value function. 
* Clearly, $$v_{k}=v_{\pi}$$ is a fixed point for this update rule because the Bellman equation for $$v_{\pi}$$ assures us of equality in this case.
*  Indeed, the sequence $$\left\{v_{k}\right\}$$ can be shown in general to converge to $$v_{\pi}$$ as $$k \rightarrow \infty$$ under the same [conditions that guarantee the existence](#existence-and-uniqueness) of $$v_{\pi}$$. 
*  We will talk more in depth about the convergence to $$v_{\pi}$$ in the end of the [section](#contraction-mapping).

### In-place implementation

To write a sequential computer program to implement iterative policy evaluation we can use two arrays, one $$v_k(s)$$, and one for, $$v_{k+1}(s)$$, and compute the new values one by one from the old values without alter the latter until the **sweep** (loop over all the states) is finished. Or, we can use one array and update the values **“in place”** that is, with each new value immediately overwriting the old one. 

* Both implementations converge to $$v_{\pi}$$
* The in-place algorithm converges faster than the two-array version. (As you might expect, because it uses new data as soon as they are available)
* For the in-place algorithm, the order in which states have their values updated during the sweep has a significant influence on the rate of convergence.
* Formally, iterative policy evaluation converges only in the limit, but in practice it must be halted short of this. (Generally stopping when $$\max_{s \in \mathcal{S}} \mid v_{k+1}(s)-v_{k}(s) \mid$$ is sufficiently small)

**TODO**: code example
{:.message}

## Policy Improvement

Once we have evaluated the value of the policy $$\pi$$, i.e. found $$v_{\pi}$$ in the prediction step, the next step is to try to **improve** that policy.

### Policy improvement theorem

The **policy improvement theorem** states that for two policies $$\pi$$ and $$\pi^{\prime}$$:

$$
q_{\pi}\left(s, \pi^{\prime}(s)\right) \geq v_{\pi}(s) \implies v_{\pi^{\prime}}(s) \geq v_{\pi}(s) \qquad \forall s \in \mathcal{S}
$$

where the strict inequality in the left side, implies the strict inequality in the right side.

The proof of the theorem consists in expanding the definition of $$q_{\pi}\left(s, \pi^{\prime}(s)\right)$$ and apply the implication of the theorem iteratively to the state-value of the next state until obtain the full definition of the return, and therefore, the definition of $$v_{\pi^{\prime}}(s)$$:

$$
\begin{aligned}
v_{\pi}(s) & \leq q_{\pi}\left(s, \pi^{\prime}(s)\right) \\
&=\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=\pi^{\prime}(s)\right] \\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, \pi^{\prime}\left(S_{t+1}\right)\right) \mid S_{t}=s\right] \\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma \mathbb{E}\left[R_{t+2}+\gamma v_{\pi}\left(S_{t+2}\right) \mid S_{t+1}, A_{t+1}=\pi^{\prime}\left(S_{t+1}\right)\right] \mid S_{t}=s\right] \\
&=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} v_{\pi}\left(S_{t+2}\right) \mid S_{t}=s\right] \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} v_{\pi}\left(S_{t+3}\right) \mid S_{t}=s\right] \\
& \vdots \\
& \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3} R_{t+4}+\cdots \mid S_{t}=s\right] \\
&=v_{\pi^{\prime}}(s)
\end{aligned}
$$

### The greedy policy

One way to improve the policy $$\pi$$, is to act **greedy** with respect to the state-values $$v_{\pi}$$ we just predicted, i.e. for each state $$s$$ take the action that leads to the next state $$s^{\prime}$$ with the maximum value $$v_{\pi}(s^{\prime})$$. We can call this new policy, the *greedy* policy $$\pi^{\prime}$$ formally defined as:

$$
\begin{aligned} 
\pi^{\prime}(s) & \doteq \underset{a}{\operatorname{argmax}}\left( q_{\pi}(s, a) \right)
\\ &=\underset{a}{\operatorname{argmax}}\left( \mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) | S_{t}=s, A_{t}=a\right] \right)
\\ &=\underset{a}{\operatorname{argmax}}\left( \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right] \right)
\end{aligned}
$$

where $$argmax$$ denotes the value of $$a$$ at which the expression that follows is maximized (with ties broken arbitrarily).

By construction, the greedy policy $$\pi^{\prime}$$ meets the conditions of the policy improvement theorem, so we know that itis as good as, or better than, the original policy. 

If at some point, the improvement stops, we that have $$\pi^{\prime} = \pi$$, and $$v_{\pi^{\prime}} = v_{\pi}$$, then, from the definition of $$\pi^{\prime}$$ we have:

$$
v_{\pi^{\prime}} =\underset{a}{\operatorname{max}}\left( \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{\pi^{\prime}}\left(s^{\prime}\right)\right] \right) =
\max _{a} \left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi^{\prime}}\left(s^{\prime}\right)\right)
$$

that it's in fact the Bellman optimality equation for $$v_{*}(s)$$. Therefore, $$v_{\pi^{\prime}}$$ must be $$v_{*}(s)$$, and $$\pi$$ and $$\pi^{\prime}$$ must be optimal policies.

Then we have that:

> The policy improvement of $$\pi$$ to $$\pi^{\prime}$$ always gives us a strictly better policy except when the $$\pi$$ is already optimal.

## Policy Iteration

Once a policy, $$\pi$$, has been improved using $$v_{\pi}$$ to yield a better policy, $$\pi^{\prime}$$, we can then compute $$v_{\pi^{\prime}}$$ and improve it again to yield an even better $$\pi^{\prime\prime}$$. We can thus obtain a sequence of monotonically improving policies and value functions:

$$
\pi_{0}\stackrel{\mathbf{E}}{\longrightarrow}
v_{\pi_{0}}\stackrel{\mathrm{I}}{\longrightarrow}
\pi_{1}\stackrel{\mathbf{E}}{\longrightarrow}
v_{\pi_{1}}\stackrel{\mathrm{I}}{\longrightarrow}
\pi_{2}\stackrel{\mathbf{E}}{\longrightarrow}
\cdots\stackrel{\mathrm{I}}{\longrightarrow}
\pi_{*}\stackrel{\mathbf{E}}{\longrightarrow}v_{\pi_{*}}
$$

where $$\stackrel{\mathbf{E}}{\longrightarrow}$$ denotes a policy evaluation, and $$\stackrel{\mathrm{I}}{\longrightarrow}$$ denotes a policy improvement.

Since a finite MDP has only a finite number of policies, this process known as **policy iteration** must **converge to an optimal policy** and an optimal value function in a finite number of iterations.

> Policy iteration solves an MDP

### Modified Policy Iteration

Policy iteration consists then of two interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement). These processes can perhaps be seen as pulling in opposite directions. Making the policy greedy, typically moves the value function away from the consistency achieved by the policy evaluation, and making the value function consistent with the policy typically causes that policy no longer to be greedy. However, if we think of these processes as two constraints or goals (e.g., as two lines in 2D space as suggested by the image in the left), driving directly toward one goal causes also some movement towards optimality (the horizontal direction), therefore, the joint action of both processes is brought closer to the overall goal of optimality, even though neither is attempting to achieve it directly.

![][policy-iteration] Bidimensional visualization of the policy iteration (left) and the modified policy iteration (right) processs.
{:.figure}

When policy iteration is implemented, each policy evaluation (itself an iterative computation), can be started with the value function for the previous policy. This typically results in a great increase in the speed of convergence of policy evaluation (presumably because the value function changes little from one policy to the next), but even with this hack, a major drawback to the DP methods is that they involve **operations over the entire state set** of the MDP. If the state set is very large, then even a single sweep **can be prohibitively expensive**. Luckily, the policy evaluation step can be truncated without losing the convergence guarantees of policy iteration, and with a great reduction in computing times, in a process known as **modified policy iteration** (image of the right).

**TODO**: code example
{:.message}

## Value Iteration

The **value iteration** algorithm, is one important special case of modified policy iteration. In particular, **policy evaluation is stopped after just one sweep** (one update of each state). 

It can be written as a particularly simple update operation that combines the policy improvement and the *one iteration policy evaluation* steps:

$$
v_{k+1}(s) = \underset{a}{\operatorname{max}}\left( \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{k}\left(s^{\prime}\right)\right] \right) =
\max _{a} \left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{k}\left(s^{\prime}\right)\right)
$$

* The update operation is identical to the [policy evaluation update](#policy-evaluation-or-prediction) except that it requires the maximum to be taken over all actions.
* Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement. 
* Another way of understanding value iteration is as simply turning the [Bellman optimality equation](#bellman-optimality-equations) into an update rule.
* Unlike policy iteration, there is no explicit policy.
* Intermediate value functions may not correspond to any policy.

**TODO**: is the last point really true? Doesn't each value funtion corresponds to the policy that is greedy w.r.t. the previous value function?
{:.message}

## Asynchronous DP

All the DP algorithms we described so far (listed in the next table), used synchronous backups over the entire state set of the MDP, that is, they require sweeps of the state set. If the state set is very large, then even a single sweep can be prohibitively expensive since the total number of states grows exponentially with the dimensionality of the state space. This major drawback is commonly known as ***the curse of dimensionality***. For example, the game of backgammon has over $$10^20$$ states. Even if we could perform the value iteration update on a million states per second, it would take over a thousand years to complete a single sweep.

![][dp-algorithms] Synchronous DP algorithms.
{:.figure}

***Asynchronous DP algorithms*** are [in-place](#in-place-implementation) DP algorithms that break this sequentiality. These algorithms **update the values of states individually in any order whatsoever**, using whatever values of other states happen to be available. The values of some states may be updated several times before the values of others are updated once. 

To converge correctly, however, an asynchronous algorithm **must continue to update the values of all the states:** it can’t ignore any state after some point in the computation. 

Asynchronous DP algorithms allow great flexibility in selecting states to update. Of course, avoiding sweeps does **not necessarily mean that we can get away with less computation**, but it gives us the posibility of select the states to which we apply updates so as to improve the algorithm’s rate of progress. We can try to order the updates to let value information propagate from state to state in an efficient way.

### Prioritised sweeping

One way is to define some measure that states how important it is to update any state of the MDP, and perform the update following that priority queue. 

In **prioritised sweeping**, that measure is just the Bellman error:

$$
\mid \max _{a \in \mathcal{A}}\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v\left(s^{\prime}\right)\right)-v(s) \mid
$$

This is we prioritize to update those states that are changing the most with respect to the previous estimation.

* Update Bellman error of affected states after each backup
* Requires knowledge of reverse dynamics (predecessor states)
* Can be implemented efficiently by maintaining a priority queue

**TODO**: update with info form the chapter 8
{:.message}

### Real-time DP

* In synchronous DP, we will likely update many states that are not even reachable from the start state
* Asynchronous algorithms also make it easier to intermix computation with **real-time interaction**
* We can run an iterative and asynchronous DP algorithm while an agent is actually experiencing the MDP
* Using the agent’s experience to determine to which states apply the updates
* Using the latest value and policy information to guide the agent’s decision making
* Focusing the DP algorithm’s updates onto parts of the state set that are most relevant to the agent.
* After each time-step $$S_t, A_t, R_{t+1}$$ we backup the state $$S_t$$ with:

$$
v\left(S_{t}\right) \leftarrow \max _{a \in \mathcal{A}}\left(\mathcal{R}_{S_{t}}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{S_{t} s^{\prime}}^{a} v\left(s^{\prime}\right)\right)
$$

**TODO**: update with info form the chapter 8
{:.message}

## Generalized Policy Iteration

As we saw in [modified policy iteration](#modified-policy-iteration), the policy evaluation step can be truncated without losing the convergence guarantees of policy iteration. If we take it to the extreme, and stop after just one sweep, we have the [value iteration](#value-iteration) algorithm. In [asynchronous DP](#asynchronous-dp) methods, the evaluation and improvement processes are interleaved at an even finer grain (in some cases a single state is updated in one process before returning to the other). However, we have seen that as long as policy evaluation and policy improvement continue to update all states, all these algorithms converge to the optimal value function and policy. Then, we use the term ***generalized policy iteration (GPI)*** to refer to the general idea of **letting these two processes interact, independent of truncation, granularity and other details**. 

**Almost all reinforcement learning methods are well described as GPI**. That is, in all we can identify two interacting processes revolving around an approximate policy and an approximate value function. One process takes the policy as given and performs some form of **policy evaluation**, by changing the value function to be more like the true value function for the policy. The other process takes the value function as given and performs some form of **policy improvement**, changing the policy to make it better, assuming that the value function is its value function**.

## Bootstrapping

All DP methods update estimates of the values of states based on estimates of the values of successor states. That is, they **update estimates on the basis of other estimates**. We call this general idea **bootstrapping**. Many reinforcement learning methods perform bootstrapping, even those that (as we will see) do not require, as DP requires, a complete and accurate model of the environment. The use of a model, and bootstrapping, are key features of RL algorithms and are separable, yet both can be mixed in interesting combinations.

## Efficiency of DP

We said earlier that (in the worst case) the time DP methods take to find an optimal policy is polynomial in the number of states and actions, even though the total number of (deterministic) policies is $$k^n$$. In this sense, DP is **exponentially faster than any direct search in policy space**.

Linear programming (LP) methods can also be used to solve MDPs (and in some cases their worst-case convergence guarantees are better than those of DP methods). but they become impractical at a much smaller number of states than do DP methods (by a factor of about 100). 

So for problems with a small state set, the efficiency DP methods does not difer much from the efficiency of LP methods, or even direct search, but as the number of states increases, the DP methods are comparatively more efficient and, at times, the only feasible ones.

Due to the ***curse of dimensionality*** (number of states growing exponentially with the dimensionality of the state space), large state sets do create difficulties. We saw that these  difficulties came mainly from the synchronous sweeps over all the state space, and that can be tackled using asynchronous DP methods. 

However, even performing the backup for one state can be too expensive, since DP methods use **full-width backups**, i.e. the iterative use of the Bellman equation sums over all actions and successor states, using the known probabilities $$p\left(s^{\prime}, r \mid s, a\right)$$. 

As we will see next, it is also possible to solve the MDP by directly **sampling** directly from it, one succesor state (an its reward), and perform the update on $$v(s)$$ only with the information obtained from this sampled succesor state (and maybe its own succesors). This way of perform the backup does not only accelerate the iterations and break the curse of dimensionality (cost of backup becomes independent of the number of states), but also presents a way to solve the MDP without requiring complete prior knowledge of it. These methods, known as **model free**, allow us to solve the RL problem when we do not have access to the $$p$$ model of the MDP, or as is commonly known, the **full RL problem**.

## Links to review

[reinforce.js karpathy](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html)
[MIT - 20. Dynamic Programming II: Text Justification, Blackjack](https://www.youtube.com/watch?v=ENyox7kNKeY)
[Dynamic Programming s](https://www.youtube.com/watch?v=DiAtV7SneRE)
<https://github.com/colinskow/move37/tree/master/dynamic_programming>

# Model Free RL

* Having access to the dynamics function $$p$$ of the MDP is not a reallistic option for the majority of the RL problems, threrefore, the model free algorithms become a very important tool, since they require only ***experience***— sequences of states, actions, and rewards sampled from **actual** or **simulated** interaction with an environment:

* Learning from **actual experience** is striking because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior. 
* Learning from **simulated experience** requires a model, but the model need only generate sample transitions, not the complete function $$p$$ that is required for DP. It is also very powerful because in surprisingly many cases it is easy to generate experience sampled according to the desired probability distributions, but infeasible to obtain the distributions in explicit form.

## Monte Carlo

* We defined the value of a state, as the return the agent can expect following a policy.
* Monte Carlo (MC) methods aproximate this expectation by averaging sampled returns from experience
* Empirical mean return instead of expected return
* To ensure that well-defined returns are available, here we define Monte Carlo methods **only for episodic tasks**. 
* Only on the completion of an episode are value estimates and policies changed.
* The **estimates for each state are independent**, do not build upon the estimates of other states.
* In other words, Monte Carlo methods **do not [bootstrap](#bootstrapping)**.

* MC methods sample and average returns for each state–action pair much like
the [bandit methods]({{ 'multi-armed-bandits' | relative_url }}) sample and average rewards for each action.
* The main difference is that now there are **multiple states**, each acting like a different bandit problem (like a [contextual bandit]({{ 'multi-armed-bandits' | relative_url }}#associative-search-contextual-bandits)) and the different bandit problems are **interrelated** (the return after taking an action in one state depends on the actions taken in later states in the same episode). 
* Because all the action selections are undergoing learning, the problem becomes **nonstationary** from the point of view of the earlier state.

* As in [dynamic programming]({{ 'dynamic-programming' | relative_url }}), first we will consider the policy evaluation process, then policy improvement, and, finally, the control problem and its solution by GPI. 
* Each of these ideas taken from DP is extended to the Monte Carlo case in which only sample experience is available.

## Policy evaluation

### State Values

* As we said, an obvious way to estimate the value of a state (its expected return) from experience is simply to **average the returns observed** after visits to that state. 
* As more returns are observed, the average should converge to the expected value. (this idea underlies all MC methods).
* We want to **estimate $$v_{\pi}(s)$$**, so given a set of episodes obtained by following $$\pi$$ and passing through (or *visiting*) $$s$$, the state $$s$$ may be visited multiple times in the same episode.
* Then we have our first difference in MC methods, **first-visit MC** method estimates $$v_{\pi}(s)$$ as the **average of the returns following first visits to $$s$$**
* Whereas the **every-visit MC** method **averages the returns following all visits to $$s$$**.
* Both methods **converge** to $$v_{\pi}(s)$$ as the number of visits (or first visits) to $$s$$ goes to infinity. 
* In the case of **first-visit MC** each return is an [i.i.d.][i.i.d.] estimate of $$v_{\pi}(s)$$ and by the [law of large numbers][law of large numbers] the sequence of averages of these estimates converges to their expected value.
* Each average is itself an unbiased estimate, and the SD of its error falls as $$1/\sqrt{n}$$, where $$n$$ is the number of returns averaged). 
* **Every-visit MC** is less straightforward, but its estimates also converge quadratically to $$v_{\pi}(s)$$ ([Singh and Sutton, 1996](https://link.springer.com/content/pdf/10.1007%2FBF00114726.pdf)).

### Action Values

* With a model, state values alone are sufficient to determine a policy*
* One simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state. 
* We did just that to define the [greedy policy](the-greedy-policy) in DP.

* **Without a model, however, state values alone are not sufficient**. 
* Thus, one of our primary goals for Monte Carlo methods is to estimate $$q_*$$
* MC methods to estimate $$q_{\pi}(s, a)$$, averages the returns following the first time (or every) in each episode that the state $$s$$ was visited and the action was taken $$a$$.

## Policy improvement (Monte Carlo Control)

* Policy improvement is again done by making the policy greedy with respect to the current value function. 
* In this case we have an action-value function, and therefore no model is needed to construct the greedy policy. 
* Policy improvemen, then is be done by constructing each $$\pi_{k+1}$$ as the greedy policy with respect to $$q_{\pi_k}$$. 
* Again, we know that the overall process converges to the optimal policy and optimal value function because of the [policy improvement theorem](#policy-improvement-theorem)

## Policy iteration

* As we stated for DP, we do not need to complete policy evaluation (sample inifite episodes in this case) to ensure convergence.
* On each evaluation step we move the value function **toward** $$q_{\pi_k}$$, but we do not expect to actually get close except over many steps. 
* One extreme form of the idea is, as we saw, [value iteration](value-iteration), in which only one iteration of iterative policy evaluation is performed between each step of policy improvement (the in-place version of value iteration is even more extreme, alternating between improvement and evaluation steps for single states).
* For MC it is natural to **alternate between evaluation and improvement on an episode-by-episode basis**. 
* After each episode, the observed returns are used for policy evaluation, and then the policy is improved at all the states visited in the episode. 

## Exploring starts

* A **complication** that appears with MC methods is that **many state-action pairs may never be visited**. 
* If we are following a **deterministic** policy, we will observe returns only for one of the actions from each state, and with no returns to average, the estimates of the other actions will not improve with experience
* To compare alternatives we need to estimate the value of **all** the actions from each state.

* One way to overcome this particular form of the [exploration-exploitation dilemma](#exploration-exploitation-dilemma), is the assumption of **exploring starts (ES)**. 
* This is, specifying that the episodes start in a state-action pair, and that every pair has a nonzero probability of being selected as the start. 
* This guarantees that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes.

### Monte Carlo ES

* A complete simple algorithm that performs policy iteration as described on the [previous section](#policy-iteration), and uses this assumption, is the called **Monte Carlo ES**. 
* In Monte Carlo ES, all the returns for each state–action pair are accumulated and averaged, irrespective of what policy was in force when they were observed.

It is easy to see that **Monte Carlo ES cannot converge to any suboptimal policy**. If it did, then the value function would eventually converge to the value function for that policy, and that in turn would cause the policy to change. Stability is achieved only when both the policy and the value function are optimal. **Convergence to this optimal fixed point seems inevitable** as the changes to the action-value function decrease over time, **but has not yet been formally proved**

### Removing ES assumption

When learning directly from actual interaction with an environment, we can not rely on the exploring starts assumption. In that case, the most common alternative approach to assuring that all state-action pairs are encountered is to consider only **stochastic policies** with a nonzero probability of selecting all actions in each state.

There are two important variants of this approach, resulting in what we call *on-policy*, and *off-policy* methods.

* **On-policy** methods attempt to evaluate or improve the **policy that is used** to make decisions
* **off-policy** methods evaluate or improve a policy **different from that used** to generate the data. 

The Monte Carlo ES method developed above is an example of an on-policy method.

## On-policy methods

In on-policy control methods the policy is generally **soft**, meaning that, $$\pi(a \mid s)>0$$ for all $$s \in \mathcal{S}$$, and all $$a \in \mathcal{A}(s)$$, but gradually shifted closer and closer to a deterministic optimal policy. Many of the methods discussed in [multi-armed bandits post]({{ 'multi-armed-bandits' | relative_url }}#exploration-exploitation-dilemma) provide mechanisms for
this. 

The on-policy method we present in this section uses **$$\varepsilon$$-greedy** policies. That is, all nongreedy actions are given the minimal probability of selection, $$\frac {\varepsilon} {\mid \mathcal{A}(s) \mid}$$, and the remaining bulk of the probability, $$(1-\varepsilon) + \frac {\varepsilon} {\mid \mathcal{A}(s) \mid}$$, is given to the greedy action. The $$\varepsilon$$-greedy policies are examples of **$$\varepsilon$$-soft** policies, defined as policies for which $$\pi(a \mid s) \geq \frac{\varepsilon}{\mid \mathcal{A}(s) \mid}$$ for all states and actions, for some $$\varepsilon>0$$.

> Among $$\varepsilon$$-soft policies, $$\varepsilon$$-greedy policies are in some sense those that are closest to greedy.

The overall idea of on-policy Monte Carlo control is still that of GPI. As in Monte Carlo ES, we use first-visit MC for the policy estimation step, but without the assumption of exploring starts, however, we can not just make the policy greedy w.r.t. the state-action values, because that would prevent further exploration of nongreedy actions. Fortunately, GPI does not require that the policy be taken all the way to a greedy policy, only that it be moved **toward** a greedy policy. In our on-policy method we will move it only to an $$\varepsilon$$-greedy policy.

### Prove of convergence

That any $$\varepsilon$$-greedy policy ($$\pi^{prime}$$) with respect to $$q_{\pi}$$ is an improvement over any $$\varepsilon$$-soft policy $$\pi$$ is assured by the [policy improvement theorem]({{ 'dynamic-programming' | relative_url }}#policy-improvement). The conditions of the policy improvement theorem apply because for any $$s \in \mathcal{S}$$:

$$
\begin{aligned} 
q_{\pi}\left(s, \pi^{\prime}(s)\right) &=\sum_{a} \pi^{\prime}(a | s) q_{\pi}(s, a) 
\\ &=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \max _{a} q_{\pi}(s, a) 
\\ & \geq \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \sum_{a} \frac{\pi(a | s)-\frac{\varepsilon}{|A(s)|}}{1-\varepsilon} q_{\pi}(s, a)
\\ &=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)-\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+\sum_{a} \pi(a | s) q_{\pi}(s, a)
\\ &=v_{\pi}(s)
\end{aligned}
$$

where the **inequality** applies because the the sum in the second term is a weighted average with nonnegative weights summing to 1, and as such it must be less than or equal to the largest number averaged.

Thus, by the policy improvement theorem, $$\pi^{\prime} \geq \pi$$, therefore:

$$
\left(\text { i.e., } v_{\pi^{\prime}}(s) \geq v_{\pi}(s) \quad \forall s \in \mathcal{S}\right)
$$

We now prove that **equality** can hold only when both $$\pi^{\prime}$$ and $$\pi$$ are **optimal** among the $$\varepsilon$$-soft policies, that is, when they are better than or equal to all other $$\varepsilon$$-soft policies.

Consider a new environment that is just like the original environment, except that we **move the requirement** of policies to be $$\varepsilon$$-soft **inside the environment**. This is, when we pick the action $$a$$ in the state $$s$$, with probability $$\varepsilon$$, the environment **repicks** the action at random, with equal probabilities.

> The best one can do in this new environment with general policies is the same as the best one could do in the original environment with $$\varepsilon$$-soft policies.

Let $$\widetilde{v}_{*}$$ and $$\widetilde{q}_{*}$$ denote the optimal value functions for the new environment. Then a policy $$\pi$$ is optimal among $$\varepsilon$$-soft policies if and only if $$v_{\pi}=\widetilde{v}_{*}$$. From the definition of $$\widetilde{v}_{*}$$ we know that it is the unique solution to:

$$
\begin{aligned} \tilde{v}_{*}(s)=&(1-\varepsilon) \max _{a} \widetilde{q}_{*}(s, a)+\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} \widetilde{q}_{*}(s, a) \\=&(1-\varepsilon) \max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma \widetilde{v}_{*}\left(s^{\prime}\right)\right] \\ &+\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma \widetilde{v}_{*}\left(s^{\prime}\right)\right] \end{aligned}
$$

When equality holds and the $$\varepsilon$$-soft policy $$\pi$$ is no longer improved, then we also know that:

$$
\begin{aligned} v_{\pi}(s)=&(1-\varepsilon) \max _{a} q_{\pi}(s, a)+\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a) \\=&(1-\varepsilon) \max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right] \\ &+\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right] \end{aligned}
$$

However, this equation is the same as the previous one, except for the substitution of $$v_{\pi}$$ for $$\widetilde{v}_{*}$$. Because $$\widetilde{v}_{*}$$ is the unique solution, it must be that:

$$
v_{\pi}=\widetilde{v}_{*}
$$

In essence, we have shown that policy iteration works for $$\varepsilon$$-soft policies. Using the natural notion of greedy policy for $$\varepsilon$$-soft policies, one is assured of improvement on every step, except when the best policy has been found among the $$\varepsilon$$-soft policies. This analysis is independent of how the action-value functions are determined at each stage, but it does assume that they are computed exactly. Now we only achieve the best policy among the $$\varepsilon$$-soft policies, but on the other hand, we have eliminated the assumption of exploring starts.

## Off-policy evaluation

The on-policy approach in the preceding section deals with the exploration-exploitation dilemma by learning action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to use **two policies**, one that is learned about and that becomes the optimal policy, called the **target policy**, and one that is more exploratory and is used to generate behavior, called the **behavior policy**. In this case we say that learning is from data “off” the target policy, and the overall process is termed **off-policy learning**.

In the off-policy evaluation problem we wish to estimate $$v_{\pi}$$ or $$q_{\pi}$$ of following the target policy $$\pi$$, but all we have are episodes following another policy $$b$$, where $$b \neq \pi$$ (if $$b = \pi$$, it would be on-policy learning). In order to use episodes from $$b$$ to estimate values for $$\pi$$, we require that every action taken under $$\pi$$ is also taken, at least occasionally, under $$b$$ (what is called the assumption of **coverage**), thar is:

$$
\pi(a | s)>0 \Rightarrow b(a | s)>0
$$

It follows from coverage that $$b$$ must be stochastic in states where it is not identical to $$\pi$$. 

In control, $$\pi$$ is typically the deterministic greedy policy with respect to the current estimate of $$q_{\pi}$$, and therefore becomes a deterministic optimal policy, while $$b$$ remains stochastic and more exploratory, for example, an $$\varepsilon$$-greedy policy. In this section, however, we consider the prediction problem, in which $$\pi$$ is unchanging and given.

### Importance Sampling

**Importance sampling**, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the **importance-sampling ratio**.

Given a starting state $$S_t$$, the probability of the subsequent state–action trajectory, $$A_{t}, S_{t+1}, A_{t+1}, \dots, S_{T}$$, occurring under any policy $$\pi$$ is:

$$
\begin{array}{l}{\operatorname{Pr}\left\{A_{t}, S_{t+1}, A_{t+1}, \ldots, S_{T} | S_{t}, A_{t : T-1} \sim \pi\right\}} \\ {\quad=\pi\left(A_{t} | S_{t}\right) p\left(S_{t+1} | S_{t}, A_{t}\right) \pi\left(A_{t+1} | S_{t+1}\right) \cdots p\left(S_{T} | S_{T-1}, A_{T-1}\right)} \\ {\quad=\prod_{k=t}^{T-1} \pi\left(A_{k} | S_{k}\right) p\left(S_{k+1} | S_{k}, A_{k}\right)}\end{array}
$$

then we define the importance-sampling ratio as:

$$
\rho_{t : T-1} \doteq \frac{\prod_{k=t}^{T-1} \pi\left(A_{k} | S_{k}\right) p\left(S_{k+1} | S_{k}, A_{k}\right)}{\prod_{k=t}^{T-1} b\left(A_{k} | S_{k}\right) p\left(S_{k+1} | S_{k}, A_{k}\right)}=\prod_{k=t}^{T-1} \frac{\pi\left(A_{k} | S_{k}\right)}{b\left(A_{k} | S_{k}\right)}
$$

where $$p$$ is the state-transition probability function of the MDP, and although the trajectory probabilities depend on these probabilities, they appear identically in both the numerator and denominator, and thus **the importance sampling ratio ends up depending only on the two policies and the sequence, not on the MDP**.

### Ordinary and weighted importance sampling

Recall that we wish to estimate the expected returns (values) under the target policy, but all we have are returns $$G_t$$ due to the behavior policy. These returns have the wrong expectation $$\mathbb{E}\left[G_{t} \mid S_{t}=s\right]=v_{b}(s)$$ and so cannot be averaged to obtain $$v_{\pi}$$. This is where importance sampling comes in. The **ratio $$\rho_{t : T}-1$$ transforms the returns to have the right expected value**:

$$
\mathbb{E}\left[\rho_{t : T-1} G_{t} \mid S_{t}=s\right]=v_{\pi}(s)
$$

Now, we can estimate $$v_{\pi}(s)$$ by simply **averaging** the transformed results (**ordinary
importance sampling**):

$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t : T(t)-1} G_{t}}{\mid\mathcal{T}(s)\mid}
$$

where $$\mathcal{T}(s)$$ is the set of all time steps in which state $$s$$ is visited (for a first-visit method, would only include time steps that were first visits to $$s$$ within their episodes), $$T(t)$$ the first time of termination following time $$t$$, and $$G_t$$ denotes the return after $$t$$ up through $$T(t)$$.

An important alternative is **weighted importance sampling**, which uses a *weighted* average, defined as:

$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t : T(t)-1} G_{t}}{\sum_{t \in \mathcal{T}(s)} \rho_{t : T(t)-1}}
$$

or zero if the denominator is zero.

To understand these two varieties of importance sampling, consider the estimates of their first-visit methods after observing a single return from state $$s$$. In the weighted-average estimate, the ratio $$$$ for the single return **cancels** and the estimate is equal to the observed return. Given that this return was the only one observed, this is a reasonable estimate, but its expectation is $$v_b(s)$$ rather than $$v_{\pi}(s)$$, and in this statistical sense it is **biased**. In contrast, the first-visit version of the ordinary estimator is always $$v_{\pi}(s)$$ in expectation (it is **unbiased**), but it can be extreme. If the importance-sampling ratio is $$n$$, the ordinary importance sampling estimate would be $$n$$ times the observed return. That is, it would be quite **far from the observed** return even though the episode’s trajectory is considered very representative of the target policy.

> Formally, the difference between the first-visit methods of the two kinds of importance
sampling is expressed in their biases and variances.

Ordinary importance sampling is unbiased whereas **weighted importance sampling is biased (though the bias converges asymptotically to zero)**. On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. **In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred**, but we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximation that we will explore in the future.

## Incremental Implementation

Monte Carlo prediction methods can be implemented incrementally, on an episode-by-episode basis, using extensions of the techniques we described [earlier for multi-armed-bandits]({{ 'multi-armed-bandits' | relative_url }}#incremental-implementation), but averaging **returns instead of rewards** in the case of **on-policy** methods, and **scaled returns** in the case of **ordinary
importance sampling off-policy** methods. The case of off-policy methods using **weighted importance sampling** requires a slightly different incremental algorithm.

Suppose we have a sequence of returns $$G_{1}, G_{2}, \ldots, G_{n-1}$$ all starting in the same state and each with a corresponding random weight $$W_{i}\left(\text{e.g., } W_{i}=\rho_{t_{i}} : T\left(t_{i}\right)-1\right)$$. We wish to form the estimate:

$$
V_{n} \doteq \frac{\sum_{k=1}^{n-1} W_{k} G_{k}}{\sum_{k=1}^{n-1} W_{k}}, \quad n \geq 2
$$

and keep it up-to-date as we obtain a single additional return $$G_n$$. In addition to keeping
track of $$V_n$$, we must maintain for each state the cumulative sum $$C_n$$ of the weights given to the first $$n$$ returns. The update rules then are:

$$
V_{n+1} \doteq V_{n}+\frac{W_{n}}{C_{n}}\left[G_{n}-V_{n}\right], \qquad n \geq 1
\\
\\ C_{n+1} \doteq C_{n}+W_{n+1}
$$

where $$C_{0} \doteq 0$$ (and $$V_1$$ is arbitrary and thus need not be specified).

## Off-policy improvement

**One example** of an algorithm of off-policy improvement can be to use weighted importance sampling for **estimating** $$\pi_{*}$$ and $$q_{*}$$, the greedy policy with respect to the estimates of $$q_{\pi}$$ as the **target** policy, and the **behavior** policy $$b$$ can be anything, but in order to assure convergence of $$\pi$$ to the optimal policy, an infinite number of returns must be obtained for each pair of state and action, and this can be assured by choosing $$b$$ to be $$\varepsilon$$-soft.

**A potential problem** is that this method learns only from the tails of episodes, when all of the remaining actions in the episode are greedy. If nongreedy actions are common, then learning will be slow, particularly for states appearing in the early portions of long episodes. Potentially, this could **greatly slow learning**. There has been insufficient experience with off-policy Monte Carlo methods to assess how serious this problem is. If it is serious, the most important way to address it is probably by incorporating [temporal-difference-learning]({{ 'temporal-difference-learning' | relative_url }}). Alternatively, if $$\gamma$$ is less than 1, then the idea developed in the [next section](#discounting-aware-importance-sampling) may also help significantly.

### Discounting-aware Importance Sampling

The off-policy methods that we have considered so far are based on forming importance sampling
weights for returns considered as unitary wholes, without taking into account the **returns’ internal structures** as sums of discounted rewards. We now briefly consider cutting-edge research ideas for using this structure to significantly reduce the variance of off-policy estimators.

For example, consider the case where episodes are long and $$\gamma$$ is significantly less than 1. For concreteness, say that episodes last **100 steps and that $$\gamma=0$$**. The return from time 0 will then be just $$G_0 = R_1$$, but its importance sampling ratio will be a product of 100 factors, $$\frac{\pi\left(A_{0} \mid S_{0}\right)}{b\left(A_{0} \mid S_{0}\right)} \frac{\pi\left(A_{1} \mid S_{1}\right)}{b\left(A_{1} \mid S_{1}\right)} \cdots \frac{\pi\left(A_{99} \mid S_{99}\right)}{b\left(A_{99} \mid S_{99}\right)}$$. In ordinary importance sampling, the return will be scaled by the entire product, but it is really only necessary to scale by the first factor ($$\frac{\pi\left(A_{0} \mid S_{0}\right)}{b\left(A_{0} \mid S_{0}\right)}$$), the other **99 factors are irrelevant** because after the first reward the return has already been determined. These later factors are all independent of the return and of expected value 1; **they do not change the expected update, but they add enormously to its variance**. In some cases they could even make the variance infinite. Let us now consider an idea for avoiding this large extraneous variance.

> The essence of the idea is to think of discounting as determining a probability of termination or, equivalently, a degree of partial termination.

For any $$\gamma \in[0,1)$$ we can think of the return $$G_0$$ as partly terminating in one step,  with probability $$1-\gamma$$, and return the reward $$R_1$$, partly terminating in two steps with probability $$(1-\gamma)\gamma$$ (terminate in the 2nd step with $$(1-\gamma)$$, and not terminate in the 1st step with $$\gamma$$), and return the reward $$R_1 + R_2$$, and so on. The partial returns here are called **flat partial returns** (where “flat” denotes the absence of discounting, and “partial” denotes that these returns do not extend all the way to termination but instead stop at $$h$$, called the **horizon** (and $$T$$ is the time of termination of the episode):

$$
\overline{G}_{t : h} \doteq R_{t+1}+R_{t+2}+\cdots+R_{h}, \quad 0 \leq t<h \leq T
$$

The conventional **full return** $$G_t$$ can be viewed as a sum of flat partial returns as suggested above as follows:

$$
\begin{aligned} G_{t} \doteq & R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots+\gamma^{T-t-1} R_{T} \\=&(1-\gamma) R_{t+1} \\ &+(1-\gamma) \gamma\left(R_{t+1}+R_{t+2}\right) \\ &+(1-\gamma) \gamma^{2}\left(R_{t+1}+R_{t+2}+R_{t+3}\right) \\ & \vdots \\ &+(1-\gamma) \gamma^{T-t-2}\left(R_{t+1}+R_{t+2}+\cdots+R_{T-1}\right) \\ &+\gamma^{T-t-1}\left(R_{t+1}+R_{t+2}+\cdots+R_{T}\right) \\ &=(1-\gamma) \sum_{h=t+1}^{T-1} \gamma^{h-t-1} \overline{G}_{t : h}+\gamma^{T-t-1} \overline{G}_{t : T} \end{aligned}
$$

Now, we can replace this new definition of the full return based in the flat partial returns, in the previous definitions of [ordinary and weighted importance sampling](#ordinary-and-weighted-importance-sampling), and obtain the ***discounting-aware*** ordinary importance sampling estimator:

$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)}\left((1-\gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t : h-1} \overline{G}_{t : h}+\gamma^{T(t)-t-1} \rho_{t : T(t)-1} \overline{G}_{t : T(t)}\right)}{|\mathcal{T}(s)|}
$$

and the *discounting-aware* weighted importance-sampling estimator:

$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)}\left((1-\gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t : h-1} \overline{G}_{t : h}+\gamma^{T(t)-t-1} \rho_{t : T(t)-1} \overline{G}_{t : T(t)}\right)}{\sum_{t \in \mathcal{T}(s)}\left((1-\gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t : h-1}+\gamma^{T(t)-t-1} \rho_{t : T(t)-1}\right)}v
$$

They take into account the discount rate but have no effect (are the same as the [non-discounting-aware estimators](#ordinary-and-weighted-importance-sampling)) if $$\gamma=1$$.

### Per-decision Importance Sampling

There is **one more way** in which the **structure of the return** as a sum of rewards **can be taken into account** in off-policy importance sampling, a way that may be able to reduce variance even in the absence of discounting (that is, even if $$\gamma=1$$).

In the [off-policy estimators](#ordinary-and-weighted-importance-sampling), each term of the sum in the numerator is itself a sum:

$$
\begin{aligned} \rho_{t : T-1} G_{t} &=\rho_{t : T-1}\left(R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-t-1} R_{T}\right) \\ &=\rho_{t : T-1} R_{t+1}+\gamma \rho_{t : T-1} R_{t+2}+\cdots+\gamma^{T-t-1} \rho_{t : T-1} R_{T} \end{aligned}
$$

Each sub-term of the equation is also a product of a random reward and a random importance-sampling ratio, for example, the first sub-term can be written as:

$$
\rho_{t : T-1} R_{t+1}=\frac{\pi\left(A_{t} | S_{t}\right)}{b\left(A_{t} | S_{t}\right)} \frac{\pi\left(A_{t+1} | S_{t+1}\right)}{b\left(A_{t+1} | S_{t+1}\right)} \frac{\pi\left(A_{t+2} | S_{t+2}\right)}{b\left(A_{t+2} | S_{t+2}\right)} \cdots \frac{\pi\left(A_{T-1} | S_{T-1}\right)}{b\left(A_{T-1} | S_{T-1}\right)} R_{t+1}
$$

Of all these factors, only the first is related with the reward $$R_{t+1}$$. All the others are for events that occurred after the reward, and in fact have no effect in expectation. This is:

$$
\mathbb{E}\left[\rho_{t : T-1} R_{t+1}\right]=\mathbb{E}\left[\rho_{t : t} R_{t+1}\right]
$$

We can say the same about the $$k$$th sub-term:

$$
\mathbb{E}\left[\rho_{t : T-1} R_{t+k}\right]=\mathbb{E}\left[\rho_{t : t+k-1} R_{t+k}\right]
$$

It follows then that the expectation of our original term can be written:

$$
\mathbb{E}\left[\rho_{t : T-1} G_{t}\right]=\mathbb{E}\left[\tilde{G}_{t}\right]
$$

where

$$
\tilde{G}_{t}=\rho_{t : t} R_{t+1}+\gamma \rho_{t : t+1} R_{t+2}+\gamma^{2} \rho_{t : t+2} R_{t+3}+\cdots+\gamma^{T-t-1} \rho_{t : T-1} R_{T}
$$

We call this idea **per-decision importance sampling**. It follows immediately that there is an alternate importance-sampling estimator, with the same unbiased expectation (in the first-visit case) using $$\tilde{G}_{t}$$:

$$
V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \tilde{G}_{t}}{|\mathcal{T}(s)|}
$$

which we might expect to sometimes be of lower variance.

**Is less clear if there is a per-decision version of weighted importance sampling**. So far, all the estimators that have been proposed for this that we know of are not consistent (that is, they do not converge to the true value with infinite data).


## Summary

> Monte Carlo methods provide an alternative policy evaluation process. Rather than use a model to compute the value of each state, they simply average many returns that start in the state.

The Monte Carlo methods presented in this chapter learn value functions and optimal policies from experience in the form of **sample episodes**. This gives them at least three kinds of advantages over DP methods:

* Can be used to learn optimal behavior **directly from interaction** with the environment, with -city, a country, etc.
* <https://www.davidsilver.uk/wp-content/uploads/2020/03/intro_RL.pdf>

## Why discounting?

We said that the concept of discounting is conceptually complex. As we said, it is introduced to avoid an infinite sum on the return, but this is not strictly necessary in the case of episodic tasks or if we use the [infinite horizon average reward criteria](https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning).

We also said that concept of discounting is introduced to model in some way the agent's uncertainty about the future, and it has proven effective for specific tasks with well-defined objectives (e.g., games), but it has never been established that fixed discounting is suitable for [general purpose use](https://arxiv.org/pdf/1902.02893.pdf) (e.g.,as a model of human preferences).

Furthermore, while the concept of discount takes inspiration from the animal world, it seems to have certain limitations when [applied to animal behavior](https://www.sciencedirect.com/science/article/pii/S0893608012002146), where in some cases, there is no learning goal.

Also a [recent study](https://arxiv.org/pdf/1910.02140.pdf) shows that discounted reinforcement learning is fundamentally incompatible with function approximation for control in continuing tasks.

As you can see, the concept of discounting on RL, it is a concept that, although it has been shown to work very well in current RL applications (generally games or tasks that can be "gamified"), is far from being fully understood, and which is the subject of current research.

## Agent frontier and the self

* In humans, where we draw this frontier is the problem of the [definition of self](https://en.wikipedia.org/wiki/Self). Realizing that this frontier (or ego) that sepparates the [*self*](https://en.wikipedia.org/wiki/%C4%80tman_(Hinduism)) from the [universe](https://en.wikipedia.org/wiki/Brahman), is just a [construction](https://en.wikipedia.org/wiki/Self-concept) and therefore that are the same thing is what in the Vedanta school of Hinduism is known as [liberation or enlightenment](https://en.wikipedia.org/wiki/Moksha).

## Approximate DP

### Multi-step approximate real-time DP (MSA-RTDP)

**TODO**: complete this section
{:.message}

"A successful  algorithms  in  this  class  is  AlphaGo  Zero  [(Silver  et  al.,  2017a)](https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge),  which  isan instance ofmulti-step approximate real-time dynamic programming(MSA-RTDP). MSA-RTDP extends the classic DP ideas by using a ‘multi-step’ lookahead,  learning the valueor policy (‘approximate’),  and operating on traces through the environment (‘real-time’) [Efroni et al. (2019)](https://arxiv.org/pdf/1909.04236.pdf) theoretically studies MSA-RTDP, showing that higher planning depthddecreases sample complexity in the real environment at the expense of increased compu-tational complexity.  Although this result is intuitive, it does show that planning may leadto better informed real-world decisions, at the expense of increased (model-based) thinkingtime.   In  addition,  iterated  planning  and  learning  may  also  lead  to  more  stable  learning,which we discuss in Sec.  5.3." extracted from [Moerland et al. (2020)](Model-based Reinforcement Learning: A Survey).

## Contraction mapping


# Links to review

* [Theory of RL - Csaba Szepesvari](https://www.youtube.com/watch?v=dHJxvSkTJHU)
* [Curiosity, unobserved rewards and function Approximation in RL - Csaba Szepesvari](https://www.youtube.com/watch?v=kbpiqNpDTkk)

# References

* [Sutton-Barto book][sutton-barto-book]
* [David Silver Course][david-silver-rl-course]
* [Szepesvari book][szepesvari-book]
* [Model-based Reinforcement Learning: A Survey][model-based-RL-a-survey]


[//]: # Images definitions

[agent-env-interaction]: /assets/images/rl/agent-env-interaction.jpeg
[rl-algorithm-categorization]: /assets/images/rl/RL_algorithm_categorization.png
[backup-diagram]: /assets/images/rl/backup-diagram.png
[bellman-backup-diagrams]: /assets/images/rl/bellman-backup-diagrams.png
[bellman-optimality-backup-diagrams]: /assets/images/rl/bellman-optimality-backup-diagrams.png
[optimality-backup-diagrams]: /assets/images/rl/optimality-backup-diagrams.png
[policy-iteration]: /assets/images/rl/policy-iteration.jpeg
[dp-algorithms]: /assets/images/rl/dp-algorithms.png

[//]: # References

[sutton-barto-book]: http://incompleteideas.net/book/RLbook2020.pdf
[szepesvari-book]: https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf
[david-silver-rl-course]: https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ
[model-based-RL-a-survey]: https://arxiv.org/pdf/2006.16712.pdf

[i.i.d.]: https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables
[law of large numbers]: https://en.wikipedia.org/wiki/Law_of_large_numbers